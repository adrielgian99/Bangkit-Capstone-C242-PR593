# -*- coding: utf-8 -*-
"""gym_and_diet_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WO8iSDPaKnJ72DmixiMg030TSQQJiT4M

# **Multi-Label Classification Using Neural Network**

### **Import Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import pathlib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Dense, Dropout

"""### **Input Dataset**"""

# Input dataset
dataset_url = 'https://raw.githubusercontent.com/adrielgian99/Bangkit-Capstone-C242-PR593/machine_learning/Clean_Dataset/gym_and_diet_recommendation_clean_dataset.csv'

df = pd.read_csv(dataset_url)

df.info()

df.describe()

"""### **Data Preprocessing**"""

# One-hot encode categorical columns
categorical_columns1 = ["gender", "hypertension_status", "diabetes_status", "fitness_goal", "fitness_type"]
onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
onehot_encoded = onehot_encoder.fit_transform(df[categorical_columns1])
onehotencoded_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(categorical_columns1))

# Concatenate one-hot encoded features with the original DataFrame
df = pd.concat([df, onehotencoded_df], axis=1)

# Label encode categorical columns
label_encoder = {}
categorical_columns2 = ["user_obesity_level", "vegetables", "protein_intake", "juice", "monday_schedule", "tuesday_schedule",
    "wednesday_schedule", "thursday_schedule", "friday_schedule",
    "saturday_schedule", "sunday_schedule"]
for column in categorical_columns2:
    label_encoder[column] = LabelEncoder()  # Create a new LabelEncoder for each column
    df[column] = label_encoder[column].fit_transform(df[column])

import json

mappings = {}

mapping_input_column = "user_obesity_level"
mappings[mapping_input_column] = {label: int(index) for index, label in enumerate(label_encoder[mapping_input_column].classes_)}

mapping_output_columns = [
    "vegetables", "protein_intake", "juice",
    "monday_schedule", "tuesday_schedule",
    "wednesday_schedule", "thursday_schedule",
    "friday_schedule", "saturday_schedule", "sunday_schedule"
]

for column in mapping_output_columns:
    mappings[column] = {label: int(index) for index, label in enumerate(label_encoder[column].classes_)}

# Save mapping to file JSON
with open("label_encoder_mapping.json", "w") as f:
    json.dump(mappings, f, indent=4)

# Normalize numerical columns
scaler = MinMaxScaler()
df[['age', 'height', 'weight', 'BMI']] = scaler.fit_transform(df[['age', 'height', 'weight', 'BMI']])

"""### **Modeling**"""

# Select input and output columns
input_columns = [
    "age", "height", "weight", "BMI", "user_obesity_level"
] + onehotencoded_df.columns.tolist()
output_columns = [
    "vegetables", "protein_intake", "juice", "monday_schedule", "tuesday_schedule",
    "wednesday_schedule", "thursday_schedule", "friday_schedule",
    "saturday_schedule", "sunday_schedule"
]

# Separate features and target
X = df[input_columns]
y = df[output_columns]

# Show X and y shape
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the processed data shapes
{
    "X_train_shape": X_train.shape,
    "X_test_shape": X_test.shape,
    "y_train_shape": y_train.shape,
    "y_test_shape": y_test.shape
}

# Build the model
input_layer = Input(shape=(X_train.shape[1],), name="input_layer")

# Build the neural network model
x = Dense(256, activation='relu')(input_layer)
x = Dropout(0.2)(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.2)(x)
x = Dense(512, activation='relu')(x)

# Adjust output layers
output_layers = {}
for col in output_columns:
    num_classes = len(df[col].unique())
    output_layers[col] = Dense(num_classes, activation='softmax', name=col)(x)

# Compile the model
model = Model(inputs=input_layer, outputs=list(output_layers.values()))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics=[['accuracy'] for _ in range(len(output_columns))])

# Summary of the model
model_summary = model.summary()

"""### **Training Model**"""

history = model.fit(
    X_train,
    [y_train.iloc[:, i].values for i in range(y_train.shape[1])],
    validation_split=0.2,
    epochs=20,
    batch_size=32
)

"""### **Evaluation Model**"""

# Evaluate the model
evaluation = model.evaluate(X_test, [y_test.iloc[:, i].values for i in range(y_test.shape[1])])

# Take data from history
history_dict = history.history

# Loss Graph
plt.figure(figsize=(12, 6))
plt.plot(history_dict['loss'], label='Training Loss', color='blue')
plt.plot(history_dict['val_loss'], label='Validation Loss', color='orange')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Accuracy Graph (each output)
output_columns_graph = [col for col in history_dict.keys() if 'accuracy' in col and 'val_' not in col]
for output in output_columns_graph:
    plt.figure(figsize=(12, 6))
    plt.plot(history_dict[output], label=f'Training {output}', color='blue')
    plt.plot(history_dict[f'val_{output}'], label=f'Validation {output}', color='orange')
    plt.title(f'Training and Validation Accuracy - {output}')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

"""### **Save Model and Preprocessor**"""

# Save the model
model.save('gym_and_diet_model.h5')

"""### **Convert Model**"""

# Install tensorflowjs
!pip install tensorflowjs

!tensorflowjs_converter --input_format=keras \
gym_and_diet_model.h5 \
./tfjs_model

"""### **Example Prediction**"""

# Load the trained model
model = load_model('gym_and_diet_model.h5')

# Example new data for prediction
new_data = pd.DataFrame({
    'gender': ['pria'],
    'age': [22],
    'height': [170],
    'weight': [95.0],
    'BMI': [32.90],
    'hypertension_status': ['tidak'],
    'diabetes_status': ['tidak'],
    'user_obesity_level': ['Obesity_Type_I'],
    'fitness_goal': ['Weight Loss'],
    'fitness_type': ['Cardio Fitness']
})

# Preprocess the new data
# One-hot encode categorical columns
categorical_columns1 = ["gender", "hypertension_status", "diabetes_status", "fitness_goal", "fitness_type"]
new_data_encoded = onehot_encoder.transform(new_data[categorical_columns1])  # Use the previously fitted encoder
new_data_encoded_df = pd.DataFrame(new_data_encoded, columns=onehot_encoder.get_feature_names_out(categorical_columns1))
new_data = pd.concat([new_data, new_data_encoded_df], axis=1)

# Label encode categorical columns
new_data["user_obesity_level"] = label_encoder["user_obesity_level"].transform(new_data["user_obesity_level"])  # Use the previously fitted encoder

# Normalize numerical columns
new_data[['age', 'height', 'weight', 'BMI']] = scaler.transform(new_data[['age', 'height', 'weight', 'BMI']])  # Use the previously fitted scaler

# Select features for prediction
X_new = new_data[input_columns]

# Predict the outputs
predictions = model.predict(X_new)

decoded_predictions = {}
for i, output in enumerate(output_columns):
    # Use the encoder specific to the current output column for inverse transform
    decoded_predictions[output] = label_encoder[output].inverse_transform([np.argmax(predictions[i])])[0]

# Print predictions
for key, value in decoded_predictions.items():
    print(f"{key}: {value}")